<!DOCTYPE html>
<html>
<head>
	
	<title>The-HMC-Algorithm</title>

	<link rel="stylesheet" type="text/css" href="PublicSummary.css">

  	<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

  


<script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'"
                }
            },
            showProcessingMessages: false
        });
</script>

</head>
<body>
	<h1> The HMC Algorithm </h1>

	<p> 
		Welcome to my MPhys project public summary, here we will give an introduction to <strong>Monte Carlo methods</strong> and the <strong>HMC</strong> algorithm, and explain a problem with it that I tried to solve during my project. This page is aimed at an audience with an A-level or equivalent knowledge of Maths and Physics. Also, for the readers interested in programming, the Python scripts used to run the simulations and generate the animations for this page, along with the HTML and CSS code for the page itself, can be found by <a href="https://github.com/FranklandJack/Public-Summary">clicking here</a>.<br/>
	</p>

	<h2> What is HMC? </h2>

	<p> 
		<strong>HMC</strong> is an acronym for <strong>Hybrid Monte Carlo</strong>  (or sometimes <strong>Hamiltonian Monte Carlo</strong>). The <strong>Hybrid/Hamiltonian</strong> title is because the algorithm borrows from <strong>Hamiltonian Dynamics</strong> (an alternative but equivalent formulation of Newton's equations), and the <strong>Monte Carlo</strong> part is because, well, it is a <strong>Monte Carlo method</strong>... which leads us to our second question.
	</p>

	<h2> What is a Monte Carlo Method? </h2>
	<p> 
		<strong>Monte Carlo methods</strong> are a huge family of algorithms with numerous applications across a massive range of subjects. Most numerical calculations are too difficult or impossible to do exactly - this is where <strong>Monte Carlo methods</strong> come in handy. Broadly speaking a <strong>Monte Carlo method</strong> works as follows; we have a set of possible values for the quantity we want to calculate, we take a random sample of that set, then we approximate the quantity as the average of our samples. It is normally a good idea to take lots of samples, otherwise your estimate on the value you are trying to calculate will likely be wrong. <br><br>

		This is all a bit abstract, it is easiest to get your head round <strong>Monte Carlo methods</strong> by looking at an example. So here is a classic - estimating <span style="color: black">$\pi$</span>. We start off drawing a circle of area <span style="color: black">$\pi$</span> centred in a square of area <span style="color: black">$4$</span>. Then we draw a random sample of points within the square. If a sample falls within the circle we count it as a <span style="color: black">1</span>, otherwise we count it as a <span style="color: black">0</span>. Then we take the average of these values, i.e. <span style="color: black">$\frac{\text{points in circle}}{\text{all points}}$</span>. Intuitively this value should be <span style="color: black">$\frac{\pi}{4}$</span> since the points within the circle estimate the area of the circle, whilst points in the square i.e. the total number of points drawn, estimate the area of the square, so multiplying our fraction by <span style="color: black">$4$</span> should give us <span style="color: black">$\pi$</span>. Play the video below to see this <strong>Monte Carlo method</strong> live in action.
		<video controls="">
			<source src="Pi.mp4" type="video/mp4">
				Your browser does not support the video tag.
		</video> <br>

		We are not taking nearly enough samples here, so the estimate of <span style="color: black">$\pi$</span> is only correct to the second significant figure. However, this animation visualises the process nicely. We can see the random points being generated, and after all the points are generated, points in the circle are counted (the ones in the circle are green, the ones outside the circle are red) and then the method described above is used to estimate <span style="color: black">$\pi$</span>. 

		 The <strong>HMC</strong>  algorithm is an example of a <strong>MCMC</strong> (<strong>Markov chain Monte Carlo</strong>) algorithm.
	</p>

	<h2> What is a Markov Chain Monte Carlo Algorithm? (and when will these definitions end?) </h2>
	<p>
		Okay, this is the last one I promise. In the normal <i>plain-old</i> <strong>Monte Carlo</strong> we saw above there is a problem. The problem is that it is slow. In the above calculation of <span style="color: black">$\pi$</span> we took <span style="color: black">$200$</span> random samples, and we saw it only got the first two figures of <span style="color: black">$\pi$</span> correct. We could take more samples, however this is slow and computationally expensive, plus there is a better way. Certain samples in our set of random samples will have more of an overall contribution to the values we are trying to calculate than others. What we can do is sample these values more frequently than others to increase the accuracy of our estimate on the values we are calculating. In practice, the way we implement this is to find a probability distribution that models the system, and draw our random samples according to this distribution, so that the samples that give more of a contribution have a higher probability of being generated under the distribution. To do this we construct a <strong> Markov chain </strong> (a chain of random samples where the probability of the next sample taking on a particular value, depends only on the value of the previous sample), and ensure that in the long time limit of this chain, the samples follow the behaviour determined by the probability distribution. We can then calculate the average of these values as before, knowing that they give dominant contributions to the estimate, and so overall we have to take fewer samples. This procedure is known as <i>importance sampling</i>.
	</p>

	<h2> How does the HMC Algorithm Work? </h2>
	<p>
		In the section above, we explained why we should use importance sampling, and sample according to a probability distribution, but how should we actually do this in practice? The <strong>HMC</strong> algorithm gives us one possible method. There is a result in statistical physics that tells us that given some energy function <span style="color: black">$E(x)$</span> for the state <span style="color: black">$x$</span> (lets keep things simple here, and just take <span style="color: black">$x$</span> to be the position of a single particle of mass <span style="color: black">$1$</span>) of our system, then the probability of our system being in that state <span style="color: black">$x$</span> is given by <span style="color: black">$P(x) \propto e^{-E\left(x\right)}$</span>. So, if we have an energy function for our system, and we want to generate samples of <span style="color: black">$x$</span> we should do so according to the probability <span style="color: black">$P(x)$</span>. Suppose for example we had a system with an energy given by <span style="color: black">$E(x) = \frac{1}{2}x^{2}$</span>, which is just the potential energy of a harmonic oscillator. Then the energy and corresponding probability look like: 
		<br> <img src="Example-Harmonic-Potential.png"> <br>
		In order to perform the <strong>HMC</strong> algorithm we introduce <i>fictitious-momenta</i> into the system. This means we introduce a momentum <span style="color: black">$p$</span> to go with our position variable <span style="color: black">$x$</span> which we wish to sample. We then define a <i>Hamiltonian</i> function which is given by <span style="color: black">$H(x,p) = \frac{p^2}{2} + E(x)$</span>.
		<br>
		 We are then ready to run the steps of the algorithm:
	</p>
		<ol>
  			<li> Provide an initial state <span style="color: black">$x$</span>. </li>
  			<li> Draw a random momentum <span style="color: black">$p$</span> from a Gaussian distribution. </li>
  			<li> Calculate the Hamiltonian <span style="color: black">$H_{\text{old}}$</span>. </li>
  			<li> Evolve the system using <i>Hamilton's equations</i>.</li>
  			<li> Calculate the Hamiltonian again <span style="color: black">$H_{\text{new}}$</span>. </li>
  			<li> Accept the new state <span style="color: black">$x$</span> as a sample, and make it the current state with a probability given by <span style="color: black">$\min\left[1,e^{\left(-H_{\text{new}}+H_{\text{old}}\right)}\right]$</span>.
  			<li> Return to step 2.
		</ol>

	<p>

		If steps above are difficult to understand do not worry; all you need to know to understand this page is that in the <strong>HMC</strong> algorithm we begin with an initial state/sample, we generate a random momentum to go with it (the Gaussian distribution is a specific random distribution that is very common in physics), we calculate the the Hamiltonian quantity defined above, then evolve the values <span style="color: black">$x$</span> and <span style="color: black">$p$</span> in time using equations that are physically equivalent to the more ubiquitous <span style="color: black">$F = ma$</span>. If the value of the Hamiltonian has decreased after the evolution of the system, we accept the new value of <span style="color: black">$x$</span> as a sample and start again, this time using the new <span style="color: black">$x$</span> as our starting state, otherwise we accept the new <span style="color: black">$x$</span> with a probability that is determined by how much the Hamiltonian increased, and for larger increases the probability is smaller. If the proposal is rejected, we sample the original <span style="color: black">$x$</span> a second time and start again using the original <span style="color: black">$x$</span>. Thinking about the Hamiltonian as the energy function of our joint <span style="color: black">$x, p$</span> system this is fairly intuitive; if the energy decreases in the system we accept the new state, and if it increases we accept it with a probability that decreases very quickly for large increases in energy. This makes sense, in the world around us physical systems tend to favour states that lower their energy.
	</p>

	<p>
		Again, this is all fairly abstract, so lets return to our example system where <span style="color: black">$E(x)=\frac{1}{2}x^2$</span> and see how it works, play the video below to see <strong>HMC</strong> in action:
		<video controls="">
			<source src="HMC-Harmonic.mp4" type="video/mp4">
				Your browser does not support the video tag.
		</video> <br>
		The black dot represents our particle moving between samples during the Hamiltonian dynamics. In the left animation we can see how its position and momentum evolve according to Hamiltonian dynamics, whilst on the right we see how the energy function of our particle changes. Accepted states are recorded in green, rejected ones in red. Notice how when a proposed state is reject the particle jumps back to its previous <span style="color: black">$x$</span> position on the x-axis, whereas when a state is accepted it starts where it left off in the <span style="color: black">$x$</span> variable. Also, note how proposed states where the energy is small, i.e. states around <span style="color: black">$x=0$</span> corresponding to the maxima in the probability in the previous diagram are on the whole accepted, whilst states with larger energies corresponding to lower probabilities are on the whole rejected; this is the importance sampling.

	</p>
		<h3> A Problem with HMC </h3>
	<p>
		Now that we understand how <strong>HMC</strong> works, lets have a look at one of its problems. Let us define an energy function <span style="color: black">$E(x) = (x^2-1)^2$</span>, this is an example of an <i>anharmonic potential</i>. Plotting the energy function and the corresponding probability as above we get:
		<br> <img src="Example-Anharmonic-Potential.png"> <br>
		So we can see the most likely regions (where the probability has its maxima) for the particle to be in, are where the energy is at its lowest (where the energy function has its minima). Play the video below to see a <strong>HMC</strong> simulation for the anharmonic potential:
		<video controls="">
			<source src="HMC-Anharmonic.mp4" type="video/mp4">
				Your browser does not support the video tag.
		</video> <br>
		For the entire simulation, the particle is stuck in the right well (the well is just the dipped region around the minima), and no samples are drawn from the well on the left. This is bad. If we were to estimate the average value of <span style="color: black">$x$</span>, this particular run would give <span style="color: black">$\bar{x} > 0 $</span> whereas since the energy function is symmetric, we know <span style="color: black">$\bar{x} = 0$</span> is the correct average value. The reason this is happening is that the two wells are separated by a region of low probability/high energy in the middle, so its very unlikely that our particle will move from one well to the other, via the Hamiltonian dynamics that evolves <span style="color: black">$x$</span> to propose samples. So how do we fix this?


	</p>
		<h3> Tempered Dynamics </h3>
	<p>
		In order to fix this problem we do something called <i>tempering</i>. This basically means we boost the momentum of the particle in the first half of the Hamiltonian dynamics, and reduce it in the second half, in the hope that we can get it over the region of low probability and into the other well. The reason this is called tempering is that it corresponds to increasing the <i>temperature</i> of the system, although the details of how this works are slightly beyond the scope of this summary, we can see this process in action in the video below:
		<video controls="">
			<source src="HMC-Anharmonic-Tempering.mp4" type="video/mp4">
				Your browser does not support the video tag.
		</video> <br>
		Much better. We can see how the particle is now moving between both wells in the energy much more frequently than it was previously; this will give a position estimate of <span style="color: black">$\bar{x}\approx 0$</span>. 

	</p>
		<h2> HMC in Quantum Simulations </h2>
	<p>
		In my MPhys project I applied <strong>HMC</strong> to the quantum harmonic and anharmonic oscillators, and used it to calculate various estimates of values in those systems. Although the implementation details are beyond the scope of this summary, <strong>Monte Carlo methods</strong> are perfect for these calculations, since solving them exactly would require you to perform infinite dimensional integrals! Whereas above we had energy functions and probabilities that depended on a single variable <span style="color: black">$x$</span>, in the quantum <strong>Monte Carlo</strong> simulations we have similar functions that could depend on hundreds or thousands of variables. However, in the case of the quantum anharmonic oscillator we still have the same problem as above, the system gets stuck in an energy well that is separated from another well by a region of low probability, and I applied the same tempering technique in an attempt to solve this problem. 
	</p>

</body>
</html>
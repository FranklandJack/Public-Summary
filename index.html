<!DOCTYPE html>
<html>
<head>
	
	<title>The-HMC-Algorithm</title>

	<link rel="stylesheet" type="text/css" href="PublicSummary.css">

  	<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

  


<script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'"
                }
            },
            showProcessingMessages: false
        });
</script>

</head>
<body>
	<h1> The HMC Algorithm </h1>

	<p> 
		Welcome to my MPhys project public summary. This page is aimed at an audience with an A-level or
		equivalent knowledge of Maths and Physics. Also, for the readers interested in programming, the Python scripts used to run the simulations and generate the animations for this page, along with the HTML and CSS code for the page itself can be found <a href="https://github.com/FranklandJack/Public-Summary"> here </a>.<br/>
	</p>

	<h2> What is HMC? </h2>

	<p> 
		<strong>HMC</strong> is an acronym for <strong>Hybrid Monte Carlo</strong>  (or sometimes <strong>Hamiltonian Monte Carlo</strong>). The <strong>Hybrid/Hamiltonian</strong> title is because the algorithm borrows from <strong>Hamiltonian Dynamics</strong> (an alternative but equivalent formulation of Newton's equations) and the <strong>Monte Carlo</strong> part is because, well, it is a <strong>Monte Carlo method</strong> ... which leads us to our second question.
	</p>

	<h2> What is a Monte Carlo Method? </h2>
	<p> 
		<strong>Monte Carlo methods</strong> are a huge family of algorithms with numerous applications across a massive range of subjects. Most numerical calculations are too difficult or impossible to do exactly - this is where <strong>Monte Carlo methods</strong> come in handy. Broadly speaking a <strong>Monte Carlo method</strong> works as follows; we have a set of possible values for the quantity we want to calculate, we take a random sample of that set, then we approximate the quantity as the average of our samples. It is normally a good idea to take lots of samples, otherwise your estimate on the value you are trying to calculate will likely be wrong. <br><br>

		This is all a bit abstract, it is easiest to get your head round <strong>Monte Carlo methods</strong> by looking at an example. So here is a classic- estimating <span style="color: black">$\pi$</span>. We start off drawing a circle of area <span style="color: black">$\pi$</span> centred in a box of area <span style="color: black">$4$</span>. Then we draw a random sample of points within the box. If a sample falls within the circle we count it as a <span style="color: black">1</span>, otherwise we count it as a <span style="color: black">0</span>. Then we take the average of these values, i.e. <span style="color: black">$\frac{\text{points in circle}}{\text{all points}}$</span>. Intuitively this value should be <span style="color: black">$\frac{\pi}{4}$</span> since the the points within the circle estimate the area of the circle, whilst points in the square i.e. all samples, estimate the area of the box. Play the video below to see this <strong>Monte Carlo method</strong> live in action.
		<video controls="">
			<source src="Pi.mp4" type="video/mp4">
				Your browser does not support the video tag.
		</video> <br>

		We are not taking nearly enough samples here, so the estimate of <span style="color: black">$\pi$</span> is only correct in the first significant figure. However, this animation visualises the process nicely. We can see the random points being generated, and after all the points are generated, points in the circle are counted (in green) and then the method described above is used to estimate <span style="color: black">$\pi$</span>. 

		 The <strong>HMC</strong>  algorithm is an example of a <strong>MCMC</strong> (<strong>Markov chain Monte Carlo</strong>) algorithm.
	</p>

	<h2> What is a Markov Chain Monte Carlo Method? (and when will these definitions end?) </h2>
	<p>
		Okay, this is the last one I promise. In the normal <i>plain-old</i> Monte Carlo we saw above there is a problem. The problem is that it is slow. In the above calculation of <span style="color: black">$\pi$</span> we took <span style="color: black">$200$</span> random samples and we saw it only got the first figure of <span style="color: black">$\pi$</span> correct. We could take more samples, however this is slow and computationally expensive, plus there is a better way. Certain samples in our set of random samples will have more of an overall contribution to the values we are trying to calculate than others. What we can choose to do is sample these values more frequently than the others to increase our estimate on the values we are calculating. In practice, the way we implement this is to find a probability distribution that models the system and draw our random samples according to this distribution. To do this we construct a Markov chain (a chain of random samples where the probability of the next sample taking on a particular value depends only on the value of the previous sample), and ensure that in the long time limit of this chain, the samples follow the behaviour ascribed by the probability distribution. We can then make calculations as before using these values, knowing that they give dominant contributions, and so overall we have to take fewer samples.
	</p>

	<h2> How does the HMC algorithm work? </h2>
	<p>
		In the section above, we explained why we should use importance sampling, and sample according to a probability distribution, but how should we actually do this in practice? The <strong>HMC</strong> algorithm gives us one possible method. There is a result in statistical physics that tells us that given some energy function <span style="color: black">$E(x)$</span> for the state <span style="color: black">$x$</span> (lets keep things simple here and just take <span style="color: black">$x$</span> to be the position of a single particle of mass $1$) of our system, then the probability of our system being in that state is given by <span style="color: black">$P(x) \propto e^{-E\left(x\right)}$</span>. So, if we have an energy function for our system, then we should take our samples as above according to the probability distribution <span style="color: black">$P(x)$</span>. Suppose for example we had a system with an energy given by <span style="color: black">$E(x) = \frac{1}{2}x^{2}$</span>, which is just the potential energy of a harmonic oscillator. Then the energy and corresponding probability distribution look like: 
		<br> <img src="Example-Harmonic-Potential.png"> <br>
		In order to perform the <strong>HMC</strong> algorithm we introduce <i>fictitious-momenta</i> into the system. This means we introduce a momentum <span style="color: black">$p$</span> to go with our position variable <span style="color: black">$x$</span> which we wish to sample. We then define a <i>Hamiltonian</i> function which is given by <span style="color: black">$H(x,p) = \frac{p^2}{2} + E(x)$</span>.
		<br>
		 We are then ready to run the steps of the algorithm:
	</p>
		<ol>
  			<li> Provide an initial state <span style="color: black">$x$</span>. </li>
  			<li> Draw random momentum <span style="color: black">$p$</span> from a Gaussian distribution. </li>
  			<li> Calculate the Hamiltonian <span style="color: black">$H_{\text{old}}$</span>. </li>
  			<li> Evolve system using <i>Hamilton's equations</i>.</li>
  			<li> Calculate Hamiltonian again <span style="color: black">$H_{\text{new}}$</span>. </li>
  			<li> Accept new state <span style="color: black">$x$</span> as sample, and make it the current state with probability given by <span style="color: black">$\min\left[1,e^{\left(-H_{\text{new}}+H_{\text{old}}\right)}\right]$</span>.
  			<li> Return to step 2.
		</ol>

	<p>

		If steps above are difficult to understand do not worry; all you need to know to understand this page is that in the <strong>HMC</strong> algorithm we begin with an initial state/sample, we generate a random momentum to go with it (the Gaussian distribution is a specific random distribution that is very common in physics), we calculate the the Hamiltonian quantity defined above, then evolve the values <span style="color: black">$x$</span> and <span style="color: black">$p$</span> in time using equations that are physically equivalent to the more ubiquitous <span style="color: black">$F = ma$</span>. If the value of the Hamiltonian has decreased after the evolution of the system we accept the new value of <span style="color: black">$x$</span> as a sample and start again, this time using the new <span style="color: black">$x$</span> as our starting state, otherwise we accept the new <span style="color: black">$x$</span> with a probability that is determined by how much the Hamiltonian increased, and for larger increases the probability is smaller. Thinking about the Hamiltonian as the energy function of our joint <span style="color: black">$x, p$</span> system this is fairly intuitive; if the energy decreases in the system we accept the new state, and if it increases we accept it with a probability that depends on how much it increased by.
	</p>

	<p>
		Again, this is all fairly abstract, so lets return to our example system where <span style="color: black">$E(x)=\frac{1}{2}x^2$</span> and see how it works, play the video below to see HMC in action:
		<video controls="">
			<source src="HMC-Harmonic.mp4" type="video/mp4">
				Your browser does not support the video tag.
		</video> <br>
		The black dot represents our particle moving between samples. In the left animation we can see how its position and momentum evolve according to Hamiltonian dynamics, whilst on the right we see how the energy of our system changes. Accepted states are recorded in green, rejected ones in red. Notice how when a proposed state is reject the particle jumps back to its previous <span style="color: black">$x$</span> position on the x-axis, whereas when a state is accepted it starts where it left off in the <span style="color: black">$x$</span> variable.

	</p>
		<h3> A Problem with HMC </h3>
	<p>
		Now that we understand how <strong>HMC</strong> works, lets have a look at one of its problems. Let us define an energy function <span style="color: black">$E(x) = (x^2-1)^2$</span>, this is an example of an <i>anharmonic potential</i>. Plotting the energy function and the corresponding probability as above we get:
		<br> <img src="Example-Anharmonic-Potential.png"> <br>
		So we can see the most likely regions (where the probability has its maxima) for the particle to be in are where the energy is at its lowest (where the energy function has its minima). As before we can run a simulation and animate it to see what happens; this is exactly the same as the case above but for the new energy function:
		<video controls="">
			<source src="HMC-Anharmonic.mp4" type="video/mp4">
				Your browser does not support the video tag.
		</video> <br>
		It seems that for most of the simulation, the particle is stuck in the right well and no samples are drawn from the well on the left. This is bad. If we were to estimate the average value of <span style="color: black">$x$</span> this particular run would give <span style="color: black">$\bar{x} > 0 $</span> whereas since the energy function is symmetric, we know <span style="color: black">$\bar{x} = 0$</span> is the correct average value. The reason this is happening is that the two wells are separated by a region of low probability/high energy in the middle, so its very unlikely that a particle could move from one well to the other via the Hamiltonian dynamics that evolves <span style="color: black">$x$</span> to propose samples. So how do we fix this?


	</p>
		<h3> Tempering Dynamics </h3>
	<p>
		In order to fix this problem we do something called <i>tempering</i>. This basically means we boost the momentum of the particle in the first half of its trajectory, and reduce it in the second half, in the hope that we can get it over the region of low probability and into the other well. The reason this is called tempering is that it corresponds to increasing the <i>temperature</i> of the system, although the details of how this works are slightly beyond the scope of this summary, we can see this process in action in the video below:
		<video controls="">
			<source src="HMC-Anharmonic-Tempering.mp4" type="video/mp4">
				Your browser does not support the video tag.
		</video> <br>
		Much better. We can see how the particle is now moving between both wells in the potential much more frequently than it was previously; this will give much better estimates on any values calculated. 

	</p>
		<h2> HMC in Quantum Simulations </h2>
	<p>
		In my MPhys project I applied <strong>HMC</strong> to the harmonic and anharmonic quantum oscillators, and used it to calculate various estimates of values in those systems. Although the implementation details are beyond the scope of this summary, Monte Carlo Methods are perfect for these calculations since analytically the calculations would require you to perform infinite dimensional integrals! Whereas above we had energy functions and probability that depended on a single variable <span style="color: black">$x$</span>, in the quantum Monte Carlo calculations we have similar functions that could depend on hundreds or thousands of variables. However, in the case of the anharmonic quantum oscillator we still have the same problem as above, the system gets stuck in a potential well that is separated from another well by a region of low probability, and I applied the same tempering technique in an attempt to solve this problem. 
	</p>

</body>
</html>